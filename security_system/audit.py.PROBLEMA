import logging
import csv
import os
from datetime import datetime, timedelta
from dotenv import load_dotenv
import pandas as pd
import json

load_dotenv('/Users/felps/airflow/config/security.env')

class AuditLogger:
    def __init__(self):
        self.logger = logging.getLogger('OlistAudit')
self.logger.setLevel(logging.INFO)

if not self.logger.handlers:
    self._setup_file_handlers()

self.audit_file = os.getenv('AUDIT_LOG_PATH')
self.system_log_file = os.getenv('SYSTEM_LOG_PATH')

if not self.audit_file or not self.system_log_file:
    raise ValueError("AUDIT_LOG_PATH ou SYSTEM_LOG_PATH não definidos em security.env")

self._init_audit_file()

def _setup_file_handlers(self):
    audit_file_handler = logging.FileHandler(
        self.audit_file,
        mode='a',
        encoding='utf-8'
        )
        audit_formatter = logging.Formatter(
            '%(asctime)s|%(levelname)s|%(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        )
        audit_file_handler.setFormatter(audit_formatter)
        self.logger.addHandler(audit_file_handler)

        system_error_handler = logging.FileHandler(
            self.system_log_file,
            mode='a'
        )
        system_error_handler.setLevel(logging.ERROR)
        system_error_formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        system_error_handler.setFormatter(system_error_formatter)
        self.logger.addHandler(system_error_handler)

    def _init_audit_file(self):
        if not os.path.exists(self.audit_file):
            try:
                with open(self.audit_file, 'w', newline='', encoding='utf-8') as f:
                    writer = csv.writer(f)
                    writer.writerow([
                        'timestamp',
                        'level',
                        'dag_id',
                        'task_id',
                        'user',
                        'action',
                        'details',
                        'compliance_status',
                        'risk_level',
                        'service',
                        'error_message',
                        'stack_trace_needed'
                    ])
                os.chmod(self.audit_file, 0o640)
                self.logger.info(f"Audit file created and secured: {self.audit_file}", extra={'action': 'FILE_INIT'})
            except Exception as e:
                self.logger.error(f"Failed to initialize audit file {self.audit_file}: {e}", exc_info=True)
                raise

    def log(self, message: str, level: str = "INFO", **kwargs):
        log_data = {
            'timestamp': datetime.now().isoformat(),
            'level': level.upper(),
            'dag_id': kwargs.get('dag_id', 'system'),
            'task_id': kwargs.get('task_id', 'system'),
            'user': kwargs.get('user', 'airflow'),
            'action': kwargs.get('action', 'AUDIT_EVENT'),
            'details': message,
            'compliance_status': kwargs.get('status', 'LGPD_OK'),
            'risk_level': kwargs.get('risk_level', 'LOW'),
            'service': kwargs.get('service', 'N/A'),
            'error_message': kwargs.get('error_message', ''),
            'stack_trace_needed': kwargs.get('stack_trace_needed', False)
        }

        try:
            with open(self.audit_file, 'a', newline='', encoding='utf-8') as f:
                writer = csv.DictWriter(f, fieldnames=log_data.keys())
                writer.writerow(log_data)
                f.flush()

            log_msg = f"{log_data['action']} | {message}"
            log_method = getattr(self.logger, level.lower(), self.logger.info)
            log_method(
                log_msg,
                extra={'metadata': {k: v for k, v in kwargs.items() if k not in log_data}}
            )

        except Exception as e:
            self.logger.error(
                f"Falha CRÍTICA na auditoria para o evento '{log_data['action']}': {str(e)}",
                exc_info=True,
                stack_info=True
            )

    def generate_report(self, start_date: str, end_date: str) -> dict:
        try:
            start_dt = datetime.fromisoformat(start_date)
            end_dt = datetime.fromisoformat(end_date)

            df = pd.read_csv(
                self.audit_file,
                parse_dates=['timestamp'],
                date_format='ISO8601'
            )

            mask = (df['timestamp'] >= start_dt) & (df['timestamp'] <= end_dt)
            period_df = df.loc[mask].copy()

            if period_df.empty:
                self.logger.info(f"Nenhum evento de auditoria encontrado para o período: {start_date} a {end_date}")
                return {"message": "Nenhum evento encontrado para o período", "periodo": f"{start_date} a {end_date}"}

            total_events = len(period_df)
            compliance_ok_count = period_df[period_df['compliance_status'] == 'LGPD_OK'].shape[0]

            report = {
                'periodo': f"{start_date} a {end_date}",
                'total_eventos': total_events,
                'distribuicao_acoes': period_df['action'].value_counts().to_dict(),
                'taxa_conformidade_lgpd': (
                    (compliance_ok_count / total_events * 100) if total_events > 0 else 0
                ).round(2),
                'eventos_risco_alto': period_df[period_df['risk_level'] == 'HIGH'].shape[0],
                'principais_violacoes': period_df[
                    period_df['compliance_status'] != 'LGPD_OK'
                ]['details'].value_counts().head(5).to_dict(),
                'usuarios_ativos': period_df['user'].nunique(),
                'detalhes_eventos_criticos': period_df[period_df['level'].isin(['ERROR', 'CRITICAL'])].to_dict(orient='records')
            }

            self._generate_authority_report(period_df)

            self.log(
                f"Relatório de auditoria gerado para o período {start_date} a {end_date}",
                action="AUDIT_REPORT_GEN",
                report_summary={
                    'total_eventos': report['total_eventos'],
                    'taxa_conformidade': report['taxa_conformidade_lgpd']
                }
            )
            return report

        except FileNotFoundError:
            self.logger.error(f"Arquivo de auditoria não encontrado: {self.audit_file}")
            return {"error": f"Audit file not found at {self.audit_file}"}
        except Exception as e:
            self.logger.error(
                f"Falha ao gerar relatório de auditoria para {start_date} a {end_date}: {str(e)}",
                exc_info=True
            )
            return {"error": str(e)}

    def _generate_authority_report(self, df: pd.DataFrame):
        columns_to_drop = ['user', 'risk_level', 'error_message', 'stack_trace_needed', 'service']

        authority_df = df.drop(columns=[col for col in columns_to_drop if col in df.columns], errors='ignore').copy()

        report_path = os.path.join(
            os.path.dirname(self.audit_file),
            f"lgpd_authority_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        )

        try:
            authority_df.to_csv(report_path, index=False, encoding='utf-8')
            os.chmod(report_path, 0o440)
            self.logger.info(
                f"Relatório para autoridades gerado: {report_path}",
                extra={'action': 'AUTHORITY_REPORT_GEN', 'sensitive_action': True}
            )
        except Exception as e:
            self.logger.error(
                f"Falha ao gerar relatório para autoridades em {report_path}: {e}",
                exc_info=True
            )

    def get_audit_data(self, days: int = 7) -> pd.DataFrame:
        try:
            df = pd.read_csv(
                self.audit_file,
                parse_dates=['timestamp'],
                date_format='ISO8601'
            )
            cutoff = datetime.now() - timedelta(days=days)
            return df[df['timestamp'] >= cutoff].copy()
        except FileNotFoundError:
            self.logger.warning(f"Arquivo de auditoria não encontrado para recuperação de dados: {self.audit_file}")
            return pd.DataFrame()
        except Exception as e:
            self.logger.error(f"Falha ao recuperar dados de auditoria: {str(e)}", exc_info=True)
            return pd.DataFrame()

    def log_operation(self, dag_id: str, task_id: str, operation: str, metadata: dict = None):
        details_msg = f"Operation: {operation}"
        if metadata:
            details_msg += f" | Metadata: {json.dumps(metadata)}"
        self.log(
            details_msg,
            action=f"OP_{operation.upper()}",
            dag_id=dag_id,
            task_id=task_id,
            details=details_msg
        )

    def log_incident(self, severity: str, dag_id: str, task_id: str, error: str, stack_trace: bool = False):
        self.log(
            f"INCIDENT DETECTED: {error}",
            level=severity.upper(),
            action="SECURITY_INCIDENT",
            dag_id=dag_id,
            task_id=task_id,
            risk_level=severity.upper(),
            compliance_status="LGPD_BREACH" if severity.upper() in ["CRITICAL", "URGENT", "HIGH"] else "LGPD_WARNING",
            error_message=error,
            stack_trace_needed=stack_trace
        )

    def log_upload(self, local_path: str, minio_path: str):
        self.log(
            f"File uploaded from {local_path} to {minio_path}",
            action="FILE_UPLOAD",
            details=f"Source: {local_path}, Destination: {minio_path}"
        )

    def log_transfer(self, object_key: str, source_bucket: str = 'N/A', dest_bucket: str = 'N/A'):
        self.log(
            f"Object '{object_key}' transferred from '{source_bucket}' to '{dest_bucket}'",
            action="OBJECT_TRANSFER",
            details=f"Key: {object_key}, Source: {source_bucket}, Dest: {dest_bucket}"
        )

    def log_validation(self, results: dict = None, success: bool = None, stats: dict = None, failed_expectations: list = None, metadata: dict = None):
        validation_status = "VALIDATION_SUCCESS" if (success if success is not None else (results and results.get('success'))) else "VALIDATION_FAILURE"
        details_msg = f"Validation Status: {validation_status}"

        if results:
            if hasattr(results, 'success'):
                details_msg += f" | Success: {results.success}"
            if hasattr(results, 'statistics'):
                details_msg += f" | Stats: {json.dumps(results.statistics)}"
            if hasattr(results, 'results'):
                failed = [r['expectation_config']['expectation_type'] for r in results['results'] if not r['success']]
                details_msg += f" | Failed Expectations: {failed}"
        else:
            if success is not None:
                details_msg += f" | Success: {success}"
            if stats:
                details_msg += f" | Stats: {json.dumps(stats)}"
            if failed_expectations:
                details_msg += f" | Failed Expectations: {json.dumps(failed_expectations)}"

        if metadata:
            details_msg += f" | Metadata: {json.dumps(metadata)}"

        self.log(
            details_msg,
            action=validation_status,
            level="INFO" if (success if success is not None else (results and results.get('success'))) else "ERROR",
            compliance_status="LGPD_OK" if (success if success is not None else (results and results.get('success'))) else "LGPD_VIOLATION",
            details=details_msg
        )
